---
title: "What does the trend of U.S. presidents' inaugrual speeches reflect?"
author: "Xiaoxiao Guo; xg2282"
date: "Jan 28, 2018"
output: html_notebook
  
---

**<font size=5>Data Description</font>**  

The data comprises of 58 inaugrual speeches, from that of George Washington to that of Donald Trump which was delivered earlier this year.

**<font size=5>Research Question</font>**  

First of all, I must admit that I am not good at politics, so I choose to tell my story just from the data itself and do not make any comment on politics.

The main question of my research is to analyse the trend of presidents' inaugrual speeches according to the 58 inaugrual speeches we have from 1789-2017 and think about what these changes reflect. Specifically, I want to tell a story from the following three aspects:

1. Change in length of sentences in presidents' inaugrual speeches
2. Change in speakers' emotions with presidents' inaugrual speeches
3. Change in topics of presidents' inaugrual speeches

In order to analyse these changes of presidents' inaugrual speeches in these years, I split the data into three parts according to the date of these speeches:  
Period 1: 1789-1865, George Washington-Abraham Lincoln (20 speeches, 14 presidents)  
Period 2: 1866-1945, Ulysses S. Grant-Franklin D. Roosevelt (20 speeches, 14 presidents)  
Period 3: 1946-2017, Harry S. Truman-Donald J. Trump (18 speeches, 12 presidents)

**It is reasonable to divide the data into such three parts because the time interval is almost same and the number of speeches and presidents in each period is close to each other.**

In this report, I will focus my study on changes among speeches of these three periods and tell my own story.

## Part I: Change in Length of Sentences

**<font size=4>First, I look at the change in length of sentences of presidents' inaugrual speeches.</font>**

### Data Collection and Processing

Step 0: Check and install needed packages. Load the libraries and functions.
```{r, message=FALSE, warning=FALSE}
packages.used = c("rvest", "tibble", "qdap","sentimentr", "gplots", "dplyr", "tm", "syuzhet", "factoextra", "beeswarm", "scales", "RColorBrewer", "RANN", "tm", "topicmodels")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# load packages
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")

source("~/Documents/GitHub/GR5243 ADS/Spring2018-Project1-xxguo28/lib/speechFuncs.R")
```

Step 1: Read in the speeches
```{r, warning=FALSE, message=FALSE}
library("wordcloud")
library("tidytext")
library("readxl")

speech.list <- read_xlsx("~/Documents/GitHub/GR5243 ADS/Spring2018-Project1-xxguo28/data/InaugurationInfo.xlsx")
speech.list$File <- paste(speech.list$File, speech.list$Term, sep = "-")
speech.list$File<- paste("inaug", speech.list$File, ".txt", sep = "")

speech.list$Period <- rep(1:3, c(20, 20, 18))

folder.path="~/Documents/GitHub/GR5243 ADS/Spring2018-Project1-xxguo28/data/InauguralSpeeches/"
speeches=list.files(path = folder.path, pattern = "*.txt")

temp <- list.files(path = folder.path, pattern="*.txt")
speech.list$fulltext <- NA
for(i in 1:length(temp)){
speech.list$fulltext[i]<-paste(readLines(paste(folder.path,speech.list$File[i], sep = "")), collapse = " ")
}
```

Step 2: Generate list of sentences
```{r, message=FALSE, warning=FALSE}
sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    # colnames(emotions)=paste0("emo.", colnames(emotions))
    # in case the word counts are zeros?
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              )
    )
  }
}

# Some non-sentences exist in raw data due to erroneous extra end-of sentence marks. 
sentence.list=
  sentence.list%>%
  filter(!is.na(word.count)) 
```

### Data Analysis --- Length of Sentences

**<font size=4>Overview of sentence length distribution by different time periods of speeches</font>**

#### Period 1 (1789-1865)
```{r}
par(mar=c(2,6,2,2))

sentence.list.sel=filter(sentence.list, Period==1)

for (i in 1:nrow(sentence.list.sel)) {
    sentence.list.sel$File[i] <- substr(sentence.list.sel$File[i], start = 6, stop = nchar(sentence.list.sel$File[i]) - 4)}

sentence.list.sel$File=factor(sentence.list.sel$File)

sentence.list.sel$FileOrdered=reorder(sentence.list.sel$File, 
                                  sentence.list.sel$word.count, 
                                  mean, 
                                  order=T)

beeswarm(word.count~FileOrdered, 
         data=sentence.list.sel,
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.45, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.sel$FileOrdered),
         las=2, xlab="Number of words in a sentence.", ylab="",
         main="Speeches during Period 1")

```

What are these short sentences?
```{r}
sentence.list%>%
  filter(Period == 1, word.count<=3)%>%
  select(sentences)
```
#### Period 2 (1866-1945)
```{r}

par(mar=c(2,6,2,2))

sentence.list.sel=filter(sentence.list, Period==2)

for (i in 1:nrow(sentence.list.sel)) {
    sentence.list.sel$File[i] <- substr(sentence.list.sel$File[i], start = 6, stop = nchar(sentence.list.sel$File[i]) - 4)}

sentence.list.sel$File=factor(sentence.list.sel$File)

sentence.list.sel$FileOrdered=reorder(sentence.list.sel$File, 
                                  sentence.list.sel$word.count, 
                                  mean, 
                                  order=T)

beeswarm(word.count~FileOrdered, 
         data=sentence.list.sel,
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.45, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.sel$FileOrdered),
         las=2, xlab="Number of words in a sentence.", ylab="",
         main="Speeches during Period 2")

```

What are these short sentences?
```{r}
sentence.list%>%
  filter(Period == 2, word.count<=3)%>%
  select(sentences)
```

#### Period 3 (1946-2017)
```{r}
par(mar=c(2,6,2,2))

sentence.list.sel=filter(sentence.list, Period==3)

for (i in 1:nrow(sentence.list.sel)) {
    sentence.list.sel$File[i] <- substr(sentence.list.sel$File[i], start = 6, stop = nchar(sentence.list.sel$File[i]) - 4)}

sentence.list.sel$File=factor(sentence.list.sel$File)

sentence.list.sel$FileOrdered=reorder(sentence.list.sel$File, sentence.list.sel$word.count, mean, order=T)

beeswarm(word.count~FileOrdered, 
         data=sentence.list.sel,
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.45, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.sel$FileOrdered),
         las=2, xlab="Number of words in a sentence.", ylab="",
         main="Speeches during Period 3")

```

What are these short sentences?
```{r}
sentence.list%>%
  filter(Period == 3, word.count<=3)%>%
  select(sentences)
```

**<font size=4>The three plots above show that the length of sentences in presendents' inaugrual speeches has become shorter and shorter.</font>**  

In Period 1, the largest number of words in a sentence is above 120, and some sentences has more than 80 words. Most sentences has words less than 60.  

In Period 2, the largest number of words in a sentence is above 100, and some sentences has more than 60 words. Most sentences has words less than 40.  

In Period 3, the largest number of words in a sentence is below 100, and some sentences has more than 40 words. Most sentences has words less than 30.

**The pattern that the length of sentences in presendents' inaugrual speeches has become shorter and shorter is consistent with the change of human's expressing behavior.**  

**<font size=4>Nowadays, people prefer to express themselves in an easier way to make their ideas clear rather than use long sentences with difficult structures.</font>** 


## Part II: Change in Emotions of Speakers

**<font size=4>I continue my research with sentiment changes of speakers.</font>**
In this part, I also use one sentence as a unit.

### Data Analysis --- Length of Sentences with Sentiment Analysis

**<font size=4>First, I want to make a further study on sentence length variation over the course of the speech, with emotions. This is a combination of sentiment change and sentence length change.</font>**

**It makes sense that I use darker colors to represent more intense emotions. In addition, blues stand for good feelings (joy, trust, anticipation) and reds stand for bad feelings (anger, disgust, fear, sadness), blueviolet stands for surprise which is neutral. I also use dark to show negative attitude and lightgreen to show positive attitude.** 
```{r}
# feelings: "anger" "anticipation" "disgust" "fear" "joy" "sadness" "surprise" "trust"  "negative" "positive" 
# corresponding colors: "firebrick", "lightblue2", "firebrick4", "firebrick2","lightblue3", "firebrick1", "blueviolet", "lightblue1", "black", "lightgreen"

par(mfrow=c(3,1), mar=c(1,0,2,0), bty="n", xaxt="n", yaxt="n", font.main=1)

f.plotsent.len1(In.list=sentence.list, InPeriod = 1)
f.plotsent.len1(In.list=sentence.list, InPeriod = 2)
f.plotsent.len1(In.list=sentence.list, InPeriod = 3)


```

**Now we can see that there are few reds in these plots, which is consistent with our intuition that presidents always express good emotions when giving speeches. Moreover, the colors are becoming lighter and lighter from Period 1 to Period 3, which means that speakers' emotions are becoming calmer and calmer.**

**<font size=4>This change is rational because people now can control their emotions better than before. In addition, the following presidents may gain some experience about how to express their emotions in speeches from the previous president. Therefore, they can express themselves in a less radical way.</font>**

**Moreover, these plots also show the pattern that the length of sentences in speeches is becoming shorter and shorter.**

### Data Analysis --- Clustering of Emotions
**<font size=4>Next, I will show a more detailed study on emotion change of speakers among three different periods.</font>**

First, I give the heatmaps of speaker's emotions in these three periods.
```{r,fig.width=1.4, fig.height=1.4}

heatmap.2(cor(sentence.list%>%filter(Period==1)%>%select(anger:trust)), 
          scale = "none", cexRow = 0.8, cexCol = 0.8,
          col = redblue(100),  margin=c(6, 6), key=F,
          trace = "none", density.info = "none")
heatmap.2(cor(sentence.list%>%filter(Period==2)%>%select(anger:trust)), 
          scale = "none", cexRow = 0.8, cexCol = 0.8,
          col = redblue(100),  margin=c(6, 6), key=F,
          trace = "none", density.info = "none")
heatmap.2(cor(sentence.list%>%filter(Period==3)%>%select(anger:trust)), 
          scale = "none", cexRow = 0.8, cexCol = 0.8,
          col = redblue(100),  margin=c(6, 6), key=F,
          trace = "none", density.info = "none")
```
**In my opinion, the extend of darkness represent the degree of correlation between two emotions.**  
**Therefore, I think the correlation between positive emotions bocome higher and the correlation between negative emotions bocome lower than before, which means <font size=4>presidents now express more positive emotions than before.</font>** 

The following plots give us a straight view of the distributions of emotions in these three periods. 
```{r, fig.width=2.7, fig.height=0.9}
par(mfrow=c(1,3))#, mar=c(1,0,2,0), bty="n", xaxt="n", yaxt="n", font.main=1)
emo.means=colMeans(sentence.list%>%filter(Period==1)%>%select(anger:trust)>0.01)
col.use=c("firebrick", "lightblue2", "firebrick4", "firebrick2",  
          "lightblue3", "firebrick1", "blueviolet", "lightblue1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Period 1")
emo.means=colMeans(sentence.list%>%filter(Period==2)%>%select(anger:trust)>0.01)
col.use=c("firebrick", "lightblue2", "firebrick4", "firebrick2",  
          "lightblue3", "firebrick1", "blueviolet", "lightblue1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Period 2")
emo.means=colMeans(sentence.list%>%filter(Period==3)%>%select(anger:trust)>0.01)
col.use=c("firebrick", "lightblue2", "firebrick4", "firebrick2",  
          "lightblue3", "firebrick1", "blueviolet", "lightblue1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Period 3")
```

**These plots give us a better view. It is clear that good feelings (blues) rank higher than bad feelings (reds) in general. And in Period 1, fear is in the third place of the emotion expression. However, in Period 2, joy become the third place of the emotion expression. In addition, the negative emotions like disgust, sadness and anger have a less weight than before.**

**<font size=4>Thus, I will say that presidents today express more positive emotions than before and this change is also reasonable.</font>**

The following plot illustrates my view clearly.
```{r, fig.height=1, fig.width=2.5}
period.summary=tbl_df(sentence.list)%>%
  group_by(Period)%>%
  summarise(
    negative=mean(negative),
    positive=mean(positive)
  )

ggplot(period.summary) +
  geom_point(mapping = aes(x = Period, y = negative, color = "negative")) +
  geom_line(mapping = aes(x = Period, y = negative, color = "negative")) +
  geom_point(mapping = aes(x = Period, y = positive, color = "positive")) +
  geom_line(mapping = aes(x = Period, y = positive, color = "positive")) +
  labs(y = "Attitude")
```
**<font size=4>From this plot we can see that presidents always express more positive attitude than negative. Meanwhile, there is a trend that they express more positive and less negative attitude than before.</font>**

## Part III: Change in Topics of Presidents' Inaugrual Speeches

### Data Collection and Processing
**<font size=4>For topic modeling, I still use one sentence as a unit. I prepare a corpus of sentence snippets as follows.</font>** 
```{r}
corpus.list=sentence.list[2:(nrow(sentence.list)-1), ]
sentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post=sentence.list$sentences[3:(nrow(sentence.list)-1)]
corpus.list$snippets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]

# Text mining
docs <- Corpus(VectorSource(corpus.list$snippets))

#Text basic processing
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))

#remove punctuation
docs <- tm_map(docs, removePunctuation)

#Strip digits
docs <- tm_map(docs, removeNumbers)

#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))

#remove whitespace
docs <- tm_map(docs, stripWhitespace)

#Stem document
docs <- tm_map(docs,stemDocument)
```

### Data Analysis --- Topic Modeling

Step 1: Gengerate document-term matrices. 

```{r}
dtm <- DocumentTermMatrix(docs)
#convert rownames to filenames 
for (i in 1:nrow(corpus.list)) {
    corpus.list$File[i] <- substr(corpus.list$File[i], start = 6, stop = nchar(corpus.list$File[i]) - 4)}
rownames(dtm) <- paste(corpus.list$File, corpus.list$sent.id, sep="_")
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]

```

Step 2: Run LDA with 10 topics  

The following table shows the number of sentences in each topic.  
```{r}
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 10

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("~/Documents/GitHub/GR5243 ADS/Spring2018-Project1-xxguo28/output/LDAGibbs",k,"DocsToTopics.csv"))

#top 20 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
write.csv(ldaOut.terms,file=paste("~/Documents/GitHub/GR5243 ADS/Spring2018-Project1-xxguo28/output/LDAGibbs",k,"TopicsToTerms.csv"))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("~/Documents/GitHub/GR5243 ADS/Spring2018-Project1-xxguo28/output/LDAGibbs",k,"TopicProbabilities.csv"))
```

I also give the top 20 most popular words in each topic. 
```{r}
ldaOut.terms
```

Based on the most popular terms, I assign a hashtag to each topic. This part is subjective because I set up these topics manually.
```{r}
topics.hash=c("Govern", "Economy" , "Responsibility", "Peace", "Unity", "Freedom", "Defense", "Reformation", "Belief", "Patriotism") 

corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]

colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
```

#### Frequencies of topics in different periods.

The following pie plots show the distributions of the topics of speeches in these three periods.
```{r, fig.width=7, fig.height=3}
topic.1 <- corpus.list$ldatopic[corpus.list$Period == 1]
topic.prop.1 <- as.integer(table(topic.1))/length(topic.1)
per1 <- round(topic.prop.1 * 100, 2)
topic.per.1  <- paste(per1, "%", sep="")
period1 <- data.frame(Percentage =topic.prop.1, Topic = topics.hash, Percent=topic.per.1)


topic.2 <- corpus.list$ldatopic[corpus.list$Period == 2]
topic.prop.2 <- as.integer(table(topic.2))/length(topic.2)
per2 <- round(topic.prop.2 * 100, 2)
topic.per.2  <- paste(per2, "%", sep="")
period2 <- data.frame(Percentage =topic.prop.2, Topic = topics.hash, Percent=topic.per.2)


topic.3 <- corpus.list$ldatopic[corpus.list$Period == 3]
topic.prop.3 <- as.integer(table(topic.3))/length(topic.3)
per3 <- round(topic.prop.3 * 100, 2)
topic.per.3  <- paste(per3, "%", sep="")
period3 <- data.frame(Percentage =topic.prop.3, Topic = topics.hash, Percent=topic.per.3)


par(mfrow=c(1,3))
# Plot the chart.
pie(topic.prop.1, labels = topic.per.1, col = rainbow(length(topic.per.1)), main = "Topic Distribution in Period 1")
legend("topright", topics.hash, cex = 0.8, fill = rainbow(length(topic.per.1)))

pie(topic.prop.2, labels = topic.per.2, col = rainbow(length(topic.per.2)), main = "Topic Distribution in Period 2")
legend("topright", topics.hash, cex = 0.8, fill = rainbow(length(topic.per.2)))

pie(topic.prop.3, labels = topic.per.3, col = rainbow(length(topic.per.3)), main = "Topic Distribution in Period 3")
legend("topright", topics.hash, cex = 0.8, fill = rainbow(length(topic.per.3)))


```
**From the plots, we know that Responsibility and Govern play important roles in Period 1, Peace and Denfense play important roles in both Period 2 and Period 3. However, Peace and Denfense cover over 2/3 topic in Period 3.** 

**<font size=4>I think this change of topics is also consistent with the development of our society, since people care more about living a peaceful life now.</font>**

## Conclusion:
**<font size=5>From my study, I will draw a conclusion that the trend of presidents' speeches from 1789 to 2017 reflects the development of our human behaviours and thoughts.</font>** 

**<font size=4>1. Sentences become shorter than before, which implies that people are pursuing an simpler way to express themselves.</font>**  

**<font size=4>2. Less radical in sentiment means people now can control their emotion well and can always keep calm. In addition, people now become more possitive than before.</font>**  

**<font size=4>3. Topic change of speeches reflects our thoughts. It shows what kind of life we are living in or we want to live in.</font>**

## Appendix: Word Cloud

**<font size=4>In the end, I want to show the word clouds of speeches in three different periods.</font>**  
(This is not part of my story, I just do this for fun.)

### Data Collection and Processing  

Step 0: Install and load libraries
```{r, message=FALSE, warning=FALSE}
packages.used=c("tm", "wordcloud", "RColorBrewer", 
                "dplyr", "tydytext")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}

library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
```

Step 1: Read in the speeches
```{r}
speeches=list.files(path = folder.path, pattern = "*.txt")
prex.out=substr(speeches, 6, nchar(speeches)-4)

ff.all.o<-Corpus(DirSource(folder.path))
```

Step 2: Text processing
```{r}
ord <- as.factor(speech.list$File)
levels(ord) <- 1:58
ff.all <- ff.all.o[as.integer(ord)]
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)

ff.1 <- ff.all[1:20]
ff.2 <- ff.all[21:40]
ff.3 <- ff.all[41:58]

tdm.1<-TermDocumentMatrix(ff.1)
tdm.2<-TermDocumentMatrix(ff.2)
tdm.3<-TermDocumentMatrix(ff.3)

tdm.tidy.1=tidy(tdm.1)
tdm.tidy.2=tidy(tdm.2)
tdm.tidy.3=tidy(tdm.3)

tdm.overall.1=summarise(group_by(tdm.tidy.1, term), sum(count))
tdm.overall.2=summarise(group_by(tdm.tidy.2, term), sum(count))
tdm.overall.3=summarise(group_by(tdm.tidy.3, term), sum(count))
```


### Data Analysis --- Wordclouds  

**<font size=4>Make wordclouds of speeches in three different periods using TF-IDF weighted document-term matrices.</font>**  

As I would like to identify interesting words for speeches in each period, I use [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to weigh each term within speeches in each period. It highlights terms that are more widely-used in a particular period. 

```{r}
dtm.1 <- DocumentTermMatrix(ff.1,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, 
                                                         normalize =FALSE),
                                         stopwords = TRUE))
ff.dtm.1=tidy(dtm.1)

dtm.2 <- DocumentTermMatrix(ff.2,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, 
                                                         normalize =FALSE),
                                         stopwords = TRUE))
ff.dtm.2=tidy(dtm.2)

dtm.3 <- DocumentTermMatrix(ff.3,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, 
                                                         normalize =FALSE),
                                         stopwords = TRUE))
ff.dtm.3=tidy(dtm.3)
```

```{r, warning=FALSE}
par(mfrow=c(1,3),mar=c(1,1,1,1))
wordcloud(ff.dtm.1$term, ff.dtm.1$count,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))
text(0.5,-0.1,"Period 1",col="black",cex=1.2)

wordcloud(ff.dtm.2$term, ff.dtm.2$count,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Reds"))
text(0.5,-0.1,"Period 2",col="black",cex=1.2)

wordcloud(ff.dtm.3$term, ff.dtm.3$count,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(9,"Greens"))
text(0.5,-0.1,"Period 3",col="black",cex=1.2)
```











